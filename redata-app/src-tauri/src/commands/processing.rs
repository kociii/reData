// æ•°æ®å¤„ç† Tauri Commands
//
// æ ¸å¿ƒå¤„ç†æµç¨‹ï¼šAI åˆ—æ˜ å°„ + æœ¬åœ°éªŒè¯å¯¼å…¥
// ä½¿ç”¨ Tauri äº‹ä»¶ç³»ç»Ÿæ¨é€è¿›åº¦

use calamine::{open_workbook_auto, Reader, Data};
use regex::Regex;
use sea_orm::{
    ActiveModelTrait, ColumnTrait, ConnectionTrait, DatabaseConnection,
    EntityTrait, QueryFilter, QueryOrder, Set, Statement,
};
use serde::Serialize;
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, LazyLock};

use super::tasks::upsert_file_progress;

/// å°†ä¸€è¡Œæ•°æ®æ ¼å¼åŒ–ä¸ºç´¢å¼•å­—ç¬¦ä¸²ï¼Œæ ¼å¼ï¼š1:åˆ—1å†…å®¹;2:åˆ—2å†…å®¹;...n:åˆ—nå†…å®¹;
fn format_row_indexed(row: &[String]) -> String {
    row.iter()
        .enumerate()
        .map(|(i, val)| format!("{}:{};", i + 1, val))
        .collect()
}

/// æ ¹æ®å­—æ®µç±»å‹è·å–è¯†åˆ«è§„åˆ™
fn get_field_type_rules(field_type: &str) -> &'static str {
    match field_type {
        "company" => "æ•°æ®åº”å«\"æœ‰é™å…¬å¸\"ã€\"æœ‰é™è´£ä»»å…¬å¸\"ã€\"è‚¡ä»½å…¬å¸\"ã€\"é›†å›¢\"ã€Incã€Ltdã€Corpã€Co.ã€LLCç­‰ä¼ä¸šå®ä½“æ ‡è¯†ï¼›åˆ—åå«\"å®¢æˆ·\"ã€\"å–å®¶\"ä½†æ•°æ®ä¸ºçº¯æ•°å­—/çº¯å­—æ¯ç¼–å·æ—¶ï¼Œæ˜¯IDåˆ—è€Œéå…¬å¸åï¼Œä¸å¾—æ˜ å°„ã€‚\n    âš ï¸ ä¸¥ç¦æ˜ å°„ï¼šçº¯æ•°å­—åˆ—ï¼ˆå¦‚IDã€ç¼–å·ã€è®¢å•å·ç­‰ï¼‰ç»ä¸èƒ½æ˜ å°„ä¸ºå…¬å¸åç§°ï¼Œå³ä½¿åˆ—åå«æœ‰\"å®¢æˆ·\"æˆ–\"å–å®¶\"ç­‰è¯è¯­",
        "phone" => "æ•°æ®åº”ä¸º11ä½æ‰‹æœºå·ï¼ˆ1å¼€å¤´ï¼‰æˆ–å›ºè¯æ ¼å¼ï¼ˆåŒºå·-å·ç ï¼‰ï¼Œçº¯æ•°å­—ä½†ä¸ç¬¦åˆæ‰‹æœº/å›ºè¯æ ¼å¼çš„ä¸å¾—æ˜ å°„",
        "email" => "æ•°æ®å¿…é¡»åŒ…å«@ç¬¦å·ï¼Œæ ¼å¼ä¸º xxx@xxx.xxx",
        "name" => "æ•°æ®é€šå¸¸ä¸º2-4ä¸ªä¸­æ–‡å­—ç¬¦æˆ–è‹±æ–‡äººåï¼›è‹¥æ•°æ®å«\"å…¬å¸\"ã€\"æœ‰é™\"ã€\"é›†å›¢\"ç­‰è¯åˆ™ä¸ºä¼ä¸šåï¼Œä¸å¾—æ˜ å°„ä¸ºå§“å",
        "address" => "æ•°æ®åº”åŒ…å«çœ/å¸‚/åŒº/è·¯/è¡—/å·/æ¥¼ç­‰åœ°å€æˆåˆ†ï¼›å•çº¯çš„åŸå¸‚åæˆ–çœä»½åä¸æ˜¯å®Œæ•´åœ°å€",
        "date" => "æ•°æ®åº”ä¸ºæ—¥æœŸæ ¼å¼å¦‚ YYYY-MM-DDã€YYYY/MM/DDã€MM/DD/YYYY ç­‰ï¼›çº¯æ•°å­—æ—¶é—´æˆ³ä¸æ˜¯æ—¥æœŸ",
        "number" => "æ•°æ®åº”ä¸ºçº¯æ•°å­—ã€æ•´æ•°æˆ–å°æ•°ï¼›å«å­—æ¯æˆ–ç‰¹æ®Šç¬¦å·çš„ç¼–å·ä¸æ˜¯æ•°å­—å­—æ®µ",
        "id_card" => "æ•°æ®åº”ä¸º15ä½çº¯æ•°å­—æˆ–18ä½ï¼ˆå‰17ä½æ•°å­—+æœ€å1ä½æ•°å­—æˆ–Xï¼‰çš„èº«ä»½è¯å·æ ¼å¼",
        "url" => "æ•°æ®å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´",
        "text" => "é€šç”¨æ–‡æœ¬å­—æ®µï¼Œåˆ—åè¯­ä¹‰åŒ¹é…å³å¯ï¼Œä½†ä¸åº”æ˜ å°„å·²è¢«å…¶ä»–ç±»å‹æ˜ç¡®æ‹’ç»çš„åˆ—",
        _ => "æ ¹æ®åˆ—åè¯­ä¹‰å’Œæ ·æœ¬æ•°æ®å†…å®¹ç»¼åˆåˆ¤æ–­"
    }
}
use tauri::{AppHandle, Emitter};
use tokio::sync::RwLock;

use crate::backend::infrastructure::{
    config::decrypt,
    persistence::models::{
        task, ProcessingTask, field,
        AiConfig as AiConfigModel, Project, record,
    },
};
use field::Model as FieldModel;
use super::ai_utils::{call_ai_stream, extract_json};
use super::ai_service::FieldDefinition;

// ============ ä»»åŠ¡æ§åˆ¶ ============

struct TaskControl {
    paused: AtomicBool,
    cancelled: AtomicBool,
}

static ACTIVE_TASKS: LazyLock<RwLock<HashMap<String, Arc<TaskControl>>>> =
    LazyLock::new(|| RwLock::new(HashMap::new()));

// ============ äº‹ä»¶ç»“æ„ ============

#[derive(Debug, Clone, Serialize, Default)]
pub struct ProcessingEvent {
    pub event: String,
    pub task_id: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_file: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_sheet: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_row: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub total_rows: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub processed_rows: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub success_count: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error_count: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub confidence: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub mappings: Option<HashMap<String, String>>,
    /// Sheet çº§åˆ«çš„æˆåŠŸè®¡æ•°ï¼ˆsheet_complete äº‹ä»¶ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sheet_success_count: Option<i32>,
    /// Sheet çº§åˆ«çš„é”™è¯¯è®¡æ•°ï¼ˆsheet_complete äº‹ä»¶ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sheet_error_count: Option<i32>,
    /// Sheet çº§åˆ«çš„æ€»è¡Œæ•°ï¼ˆsheet_complete äº‹ä»¶ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sheet_total_rows: Option<i32>,
}

impl ProcessingEvent {
    fn emit(&self, app: &AppHandle) {
        let _ = app.emit("processing-progress", self);
    }
}

// ============ å“åº”ç»“æ„ ============

#[derive(Debug, Serialize)]
pub struct StartProcessingResponse {
    pub task_id: String,
    pub batch_number: String,
    pub project_id: i32,
    pub status: String,
    pub source_files: Vec<String>,
}

// ============ è¾…åŠ©å‡½æ•° ============

fn data_to_string(data: &Data) -> String {
    match data {
        Data::Int(i) => i.to_string(),
        Data::Float(f) => {
            if *f == (*f as i64) as f64 {
                (*f as i64).to_string()
            } else {
                f.to_string()
            }
        }
        Data::String(s) => s.clone(),
        Data::Bool(b) => b.to_string(),
        Data::DateTime(dt) => dt.to_string(),
        Data::DateTimeIso(s) => s.clone(),
        Data::DurationIso(s) => s.clone(),
        Data::Error(e) => format!("#ERR:{:?}", e),
        Data::Empty => String::new(),
    }
}

fn validate_value(value: &str, validation_rule: Option<&str>) -> bool {
    if value.trim().is_empty() {
        return true; // ç©ºå€¼é€šè¿‡ï¼ˆç”± required å­—æ®µå¤„ç†ï¼‰
    }
    if let Some(rule) = validation_rule {
        if let Ok(re) = Regex::new(rule) {
            return re.is_match(value);
        }
    }
    true
}

/// æ ¹æ®å­—æ®µç±»å‹æ¸…ç†æ•°æ®å€¼
///
/// æ¸…ç†è§„åˆ™ï¼š
/// - é€šç”¨ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦
/// - phone: ä»…ä¿ç•™æ•°å­—å’Œ + å·
/// - email: å»é™¤ç©ºæ ¼ã€æ¢è¡Œï¼Œè½¬å°å†™
/// - text/å…¶ä»–: å‹ç¼©è¿ç»­ç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
fn clean_value(value: &str, field_type: &str) -> String {
    // ç¬¬ä¸€æ­¥ï¼šé€šç”¨æ¸…ç† - å»é™¤é¦–å°¾ç©ºç™½å’Œæ§åˆ¶å­—ç¬¦
    let mut cleaned = value
        .chars()
        .map(|c| match c {
            '\r' | '\n' | '\t' => ' ',  // æ¢è¡Œã€åˆ¶è¡¨ç¬¦è½¬ä¸ºç©ºæ ¼
            c if c.is_control() => ' ', // å…¶ä»–æ§åˆ¶å­—ç¬¦è½¬ä¸ºç©ºæ ¼
            c => c,
        })
        .collect::<String>();

    // æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œç‰¹å®šæ¸…ç†
    match field_type {
        "phone" => {
            // æ‰‹æœºå·ï¼šä»…ä¿ç•™æ•°å­—å’Œ + å·
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_digit() || *c == '+')
                .collect();
        }
        "company" => {
            // å…¬å¸åç§°ï¼šå‹ç¼©ç©ºç™½ï¼›è‹¥æ¸…ç†åä¸ºçº¯æ•°å­—ï¼ˆå¦‚ IDã€ç¼–å·ï¼‰ï¼Œè§†ä¸ºæ— æ•ˆå€¼è¿”å›ç©º
            let mut result = String::new();
            let mut prev_space = false;
            for c in cleaned.chars() {
                if c.is_whitespace() {
                    if !prev_space { result.push(' '); prev_space = true; }
                } else {
                    result.push(c);
                    prev_space = false;
                }
            }
            cleaned = result.trim().to_string();
            // çº¯æ•°å­—ï¼ˆå«ç©ºæ ¼åˆ†éš”ï¼‰ä¸æ˜¯å…¬å¸åç§°ï¼Œæ¸…ç©º
            if !cleaned.is_empty() && cleaned.chars().all(|c| c.is_ascii_digit() || c.is_whitespace()) {
                cleaned = String::new();
            }
        }
        "email" => {
            // é‚®ç®±ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼ï¼Œè½¬å°å†™
            cleaned = cleaned.chars().filter(|c| !c.is_whitespace()).collect();
            cleaned = cleaned.to_lowercase();
        }
        "number" | "id_card" => {
            // æ•°å­—ã€èº«ä»½è¯ï¼šä»…ä¿ç•™æ•°å­—å’Œå­—æ¯
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_alphanumeric())
                .collect();
        }
        "date" => {
            // æ—¥æœŸï¼šå»é™¤ç©ºæ ¼ï¼Œä¿ç•™æ•°å­—ã€æ—¥æœŸåˆ†éš”ç¬¦
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_digit() || matches!(c, '-' | '/' | '.' | ':'))
                .collect();
        }
        _ => {
            // é»˜è®¤æ–‡æœ¬ç±»å‹ï¼šå‹ç¼©è¿ç»­ç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
            let mut result = String::new();
            let mut prev_space = false;
            for c in cleaned.chars() {
                if c.is_whitespace() {
                    if !prev_space {
                        result.push(' ');
                        prev_space = true;
                    }
                } else {
                    result.push(c);
                    prev_space = false;
                }
            }
            cleaned = result;
        }
    }

    // æœ€åå†æ¬¡ trim
    cleaned.trim().to_string()
}

// ============ Tauri Commands ============

/// å¼€å§‹å¤„ç†æ–‡ä»¶
#[tauri::command]
pub async fn start_processing(
    app: AppHandle,
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    project_id: i32,
    file_paths: Vec<String>,
    ai_config_id: Option<i32>,
) -> Result<StartProcessingResponse, String> {
    // è·å–æ•°æ®åº“è¿æ¥çš„å…‹éš†
    let db_conn = db.inner().clone();

    // 1. éªŒè¯é¡¹ç›®
    let project = Project::find_by_id(project_id)
        .one(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("é¡¹ç›® {} ä¸å­˜åœ¨", project_id))?;

    // 2. è·å–å­—æ®µå®šä¹‰
    let fields = field::Entity::find()
        .filter(field::Column::ProjectId.eq(project_id))
        .filter(field::Column::IsDeleted.eq(false))
        .order_by(field::Column::DisplayOrder, sea_orm::Order::Asc)
        .all(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if fields.is_empty() {
        return Err("é¡¹ç›®æ²¡æœ‰å®šä¹‰å­—æ®µ".to_string());
    }

    // 3. è·å– AI é…ç½®
    let ai_config = if let Some(config_id) = ai_config_id {
        AiConfigModel::find_by_id(config_id)
            .one(db_conn.as_ref())
            .await
            .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
            .ok_or_else(|| format!("AI é…ç½® {} ä¸å­˜åœ¨", config_id))?
    } else {
        // ä½¿ç”¨é»˜è®¤é…ç½®
        let configs = AiConfigModel::find()
            .all(db_conn.as_ref())
            .await
            .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
        configs.into_iter()
            .find(|c| c.is_default)
            .ok_or_else(|| "æ²¡æœ‰é»˜è®¤ AI é…ç½®".to_string())?
    };

    // 4. è§£å¯† API Key
    let api_key = decrypt(&ai_config.api_key)
        .map_err(|e| format!("è§£å¯†å¤±è´¥: {}", e))?;

    // 5. åˆ›å»ºä»»åŠ¡
    let task_id = uuid::Uuid::new_v4().to_string();
    let now = chrono::Utc::now();

    // ç”Ÿæˆ batch_number
    let date_str = now.format("%Y%m%d").to_string();
    let count = ProcessingTask::find()
        .filter(task::Column::ProjectId.eq(project_id))
        .filter(task::Column::BatchNumber.starts_with(&format!("BATCH_{}", date_str)))
        .all(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .len();
    let batch_number = format!("BATCH_{}_{:03}", date_str, count + 1);

    // åˆ›å»ºä»»åŠ¡è®°å½•
    // æå–æºæ–‡ä»¶ååˆ—è¡¨
    let source_file_names: Vec<String> = file_paths
        .iter()
        .map(|p| {
            std::path::Path::new(p)
                .file_name()
                .map(|s| s.to_string_lossy().to_string())
                .unwrap_or_else(|| "unknown".to_string())
        })
        .collect();
    let source_files_json = serde_json::to_string(&source_file_names)
        .unwrap_or_else(|_| "[]".to_string());

    let new_task = task::ActiveModel {
        id: Set(task_id.clone()),
        project_id: Set(project_id),
        status: Set("processing".to_string()),
        total_files: Set(file_paths.len() as i32),
        processed_files: Set(0),
        total_rows: Set(0),
        processed_rows: Set(0),
        success_count: Set(0),
        error_count: Set(0),
        batch_number: Set(Some(batch_number.clone())),
        source_files: Set(Some(source_files_json)),
        created_at: Set(now),
        updated_at: Set(None),
    };

    new_task
        .insert(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    // 6. æ³¨å†Œä»»åŠ¡æ§åˆ¶
    let control = Arc::new(TaskControl {
        paused: AtomicBool::new(false),
        cancelled: AtomicBool::new(false),
    });
    {
        let mut tasks = ACTIVE_TASKS.write().await;
        tasks.insert(task_id.clone(), control.clone());
    }

    // 7. å¯åŠ¨åå°å¤„ç†
    let app_for_spawn = app.clone();
    let project_clone = project.clone();
    let fields_clone = fields.clone();
    let api_url = ai_config.api_url.clone();
    let api_key_clone = api_key.clone();
    let model_name = ai_config.model_name.clone();
    let temperature = ai_config.temperature;
    let max_tokens = ai_config.max_tokens;
    let task_id_clone = task_id.clone();

    tokio::spawn(async move {
        let task_id_inner = task_id_clone;
        let result = process_files(
            app_for_spawn,
            db_conn.clone(),
            &task_id_inner,
            &project_clone,
            &fields_clone,
            &file_paths,
            &api_url,
            &api_key_clone,
            &model_name,
            temperature,
            max_tokens,
            control.clone(),
        ).await;

        // æ¸…ç†ä»»åŠ¡æ§åˆ¶
        {
            let mut tasks = ACTIVE_TASKS.write().await;
            tasks.remove(&task_id_inner);
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        if let Err(e) = result {
            let _ = update_task_error(&db_conn.clone(), &task_id_inner, &e).await;
            let event = ProcessingEvent {
                event: "error".to_string(),
                task_id: task_id_inner.clone(),
                message: Some(e),
                ..Default::default()
            };
            event.emit(&app);
        }
    });

    Ok(StartProcessingResponse {
        task_id,
        batch_number,
        project_id,
        status: "processing".to_string(),
        source_files: source_file_names,
    })
}

async fn update_task_error(db: &Arc<DatabaseConnection>, task_id: &str, _error: &str) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if let Some(task) = task {
        let mut active: task::ActiveModel = task.into();
        active.status = Set("error".to_string());
        active.updated_at = Set(Some(chrono::Utc::now()));
        active.error_count = Set(1);
        active.updated_at = Set(Some(chrono::Utc::now()));
        active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
    }
    Ok(())
}

async fn process_files(
    app: AppHandle,
    db: Arc<DatabaseConnection>,
    task_id: &str,
    project: &crate::backend::infrastructure::persistence::models::project::Model,
    fields: &[FieldModel],
    file_paths: &[String],
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    control: Arc<TaskControl>,
) -> Result<(), String> {
    let mut total_rows = 0i32;
    let mut processed_rows = 0i32;
    let mut success_count = 0i32;
    let mut error_count = 0i32;

    // è·å–å»é‡å­—æ®µ
    let dedup_fields: Vec<i32> = if project.dedup_enabled {
        fields.iter()
            .filter(|f| f.is_dedup_key)
            .map(|f| f.id)
            .collect()
    } else {
        vec![]
    };

    for (file_idx, file_path) in file_paths.iter().enumerate() {
        // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if control.cancelled.load(Ordering::SeqCst) {
            update_task_status(&db, task_id, "cancelled".to_string()).await?;
            return Ok(());
        }

        // ç­‰å¾…æ¢å¤
        while control.paused.load(Ordering::SeqCst) {
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            if control.cancelled.load(Ordering::SeqCst) {
                update_task_status(&db, task_id, "cancelled".to_string()).await?;
                return Ok(());
            }
        }

        let file_name = std::path::Path::new(file_path)
            .file_name()
            .map(|s| s.to_string_lossy().to_string())
            .unwrap_or_else(|| "unknown".to_string());

        // å‘é€æ–‡ä»¶å¼€å§‹äº‹ä»¶
        ProcessingEvent {
            event: "file_start".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.clone()),
            message: Some(format!("å¼€å§‹å¤„ç†æ–‡ä»¶: {}", file_name)),
            ..Default::default()
        }.emit(&app);

        // æŒä¹…åŒ–ï¼šåˆ›å»ºæ–‡ä»¶è¿›åº¦è®°å½•
        let _ = upsert_file_progress(
            &db,
            task_id,
            &file_name,
            None,  // sheet_name ä¸ºç©ºè¡¨ç¤ºæ–‡ä»¶çº§åˆ«
            Some("processing"),
            None,  // sheet_phase
            None,  // ai_confidence
            None,  // mapping_count
            None,  // success_count
            None,  // error_count
            None,  // total_rows
            None,  // error_message
        ).await;

        // å¤„ç†æ–‡ä»¶
        let result = process_single_file(
            &app,
            &db,
            task_id,
            file_path.clone(),
            &file_name,
            fields,
            api_url,
            api_key,
            model_name,
            temperature,
            max_tokens,
            &dedup_fields,
            project.dedup_enabled,
            &control,
        ).await;

        match result {
            Ok((rows, success, errors)) => {
                total_rows += rows;
                processed_rows += rows;
                success_count += success;
                error_count += errors;

                // æŒä¹…åŒ–ï¼šæ›´æ–°æ–‡ä»¶å®ŒæˆçŠ¶æ€
                let _ = upsert_file_progress(
                    &db,
                    task_id,
                    &file_name,
                    None,
                    Some("done"),
                    None,  // sheet_phase
                    None,  // ai_confidence
                    None,  // mapping_count
                    Some(success),
                    Some(errors),
                    Some(rows),
                    None,  // error_message
                ).await;
            }
            Err(e) => {
                error_count += 1;

                // æŒä¹…åŒ–ï¼šæ›´æ–°æ–‡ä»¶é”™è¯¯çŠ¶æ€
                let _ = upsert_file_progress(
                    &db,
                    task_id,
                    &file_name,
                    None,
                    Some("error"),
                    None,  // sheet_phase
                    None,  // ai_confidence
                    None,  // mapping_count
                    None,  // success_count
                    None,  // error_count
                    None,  // total_rows
                    Some(&e),
                ).await;

                ProcessingEvent {
                    event: "error".to_string(),
                    task_id: task_id.to_string(),
                    current_file: Some(file_name.clone()),
                    message: Some(e),
                    ..Default::default()
                }.emit(&app);
            }
        }

        // æ›´æ–°ä»»åŠ¡è¿›åº¦
        update_task_progress(&db, task_id, (file_idx + 1) as i32, total_rows, processed_rows, success_count, error_count).await?;

        // å‘é€æ–‡ä»¶å®Œæˆäº‹ä»¶
        ProcessingEvent {
            event: "file_complete".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.clone()),
            processed_rows: Some(processed_rows),
            success_count: Some(success_count),
            error_count: Some(error_count),
            message: Some(format!("æ–‡ä»¶å¤„ç†å®Œæˆ: {} è¡Œ", processed_rows)),
            ..Default::default()
        }.emit(&app);
    }

    // æ›´æ–°ä»»åŠ¡ä¸ºå®Œæˆ
    update_task_status(&db, task_id, "completed".to_string()).await?;

    // å‘é€å®Œæˆäº‹ä»¶
    ProcessingEvent {
        event: "completed".to_string(),
        task_id: task_id.to_string(),
        processed_rows: Some(processed_rows),
        success_count: Some(success_count),
        error_count: Some(error_count),
        message: Some(format!("å¤„ç†å®Œæˆ: æˆåŠŸ {} è¡Œ, å¤±è´¥ {} è¡Œ", success_count, error_count)),
        ..Default::default()
    }.emit(&app);

    Ok(())
}

async fn process_single_file(
    app: &AppHandle,
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    file_path: String,
    file_name: &str,
    fields: &[FieldModel],
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    dedup_fields: &[i32],
    dedup_enabled: bool,
    control: &Arc<TaskControl>,
) -> Result<(i32, i32, i32), String> {
    let mut total_rows = 0i32;
    let mut success_count = 0i32;
    let error_count = 0i32;

    // ä½¿ç”¨ spawn_blocking è¯»å– Excel å¹¶å¤„ç†æ‰€æœ‰ sheets
    let result = tokio::task::spawn_blocking(move || {
        let mut workbook = open_workbook_auto(file_path)
            .map_err(|e| format!("æ— æ³•æ‰“å¼€æ–‡ä»¶: {}", e))?;
        let sheet_names = workbook.sheet_names().to_vec();
        let mut all_rows: HashMap<String, Vec<Vec<String>>> = HashMap::new();

        for sheet_name in &sheet_names {
            let range = workbook.worksheet_range(sheet_name)
                .map_err(|e| format!("æ— æ³•è¯»å– Sheet: {}", e))?;
            let rows: Vec<Vec<String>> = range
                .rows()
                .map(|row| row.iter().map(data_to_string).collect())
                .collect();
            all_rows.insert(sheet_name.clone(), rows);
        }

        Ok::<_, String>((sheet_names, all_rows))
    })
    .await
    .map_err(|e| format!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e))?
    .map_err(|e| format!("æ— æ³•æ‰“å¼€æ–‡ä»¶: {}", e))?;

    let (sheet_names, mut all_rows) = result;

    for sheet_name in sheet_names {
        // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if control.cancelled.load(Ordering::SeqCst) {
            return Ok((total_rows, success_count, error_count));
        }

        // è®°å½• Sheet å¼€å§‹æ—¶çš„åŸºçº¿å€¼ï¼ˆç”¨äºè®¡ç®—å½“å‰ Sheet çš„å¢é‡ï¼‰
        let sheet_start_total = total_rows;
        let sheet_start_success = success_count;
        let sheet_start_error = error_count;

        // å‘é€ Sheet å¼€å§‹äº‹ä»¶
        ProcessingEvent {
            event: "sheet_start".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.to_string()),
            current_sheet: Some(sheet_name.clone()),
            message: Some(format!("å¼€å§‹å¤„ç† Sheet: {}", sheet_name)),
            ..Default::default()
        }.emit(app);

        // æŒä¹…åŒ–ï¼šåˆ›å»º Sheet è¿›åº¦è®°å½•
        let _ = upsert_file_progress(
            db,
            task_id,
            file_name,
            Some(&sheet_name),
            None,  // file_phase ä¸å˜
            Some("ai_analyzing"),
            None,  // ai_confidence
            None,  // mapping_count
            None,  // success_count
            None,  // error_count
            None,  // total_rows
            None,  // error_message
        ).await;

        // è·å–å·²è¯»å–çš„ Sheet æ•°æ®
        let rows_data = match all_rows.remove(&sheet_name) {
            Some(rows) => rows,
            None => {
                // Sheet æ•°æ®ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå®Œæˆï¼ˆ0 è¡Œï¼‰
                let _ = upsert_file_progress(
                    db, task_id, file_name, Some(&sheet_name),
                    None, Some("done"), None, None,
                    Some(0), Some(0), Some(0), None,
                ).await;
                ProcessingEvent {
                    event: "sheet_complete".to_string(),
                    task_id: task_id.to_string(),
                    current_file: Some(file_name.to_string()),
                    current_sheet: Some(sheet_name.clone()),
                    sheet_success_count: Some(0),
                    sheet_error_count: Some(0),
                    sheet_total_rows: Some(0),
                    message: Some(format!("Sheet {} æ— æ•°æ®", sheet_name)),
                    ..Default::default()
                }.emit(app);
                continue;
            }
        };

        if rows_data.is_empty() {
            // Sheet ä¸ºç©ºï¼Œæ ‡è®°ä¸ºå®Œæˆï¼ˆ0 è¡Œï¼‰
            let _ = upsert_file_progress(
                db, task_id, file_name, Some(&sheet_name),
                None, Some("done"), None, None,
                Some(0), Some(0), Some(0), None,
            ).await;
            ProcessingEvent {
                event: "sheet_complete".to_string(),
                task_id: task_id.to_string(),
                current_file: Some(file_name.to_string()),
                current_sheet: Some(sheet_name.clone()),
                sheet_success_count: Some(0),
                sheet_error_count: Some(0),
                sheet_total_rows: Some(0),
                message: Some(format!("Sheet {} æ— æ•°æ®ï¼Œè·³è¿‡", sheet_name)),
                ..Default::default()
            }.emit(app);
            continue;
        }

        // AI åˆ†æåˆ—æ˜ å°„
        ProcessingEvent {
            event: "ai_analyzing".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            message: Some("AI åˆ†æåˆ—æ˜ å°„...".to_string()),
            ..Default::default()
        }.emit(app);

        // æ„å»ºå­—æ®µå®šä¹‰
        let field_defs: Vec<FieldDefinition> = fields.iter().map(|f| FieldDefinition {
            field_name: f.field_name.clone(),
            field_label: f.field_label.clone(),
            field_type: f.field_type.clone(),
            additional_requirement: f.additional_requirement.clone(),
            extraction_hint: f.extraction_hint.clone(),
        }).collect();

        // AI åˆ†æï¼ˆæµå¼ï¼‰
        let app_clone = app.clone();
        let task_id_clone = task_id.to_string();
        let sheet_name_clone = sheet_name.clone();

        // æ„å»ºè¯·æ±‚æç¤ºï¼ˆç”¨äºæ˜¾ç¤ºï¼‰- åªå–å‰ 5 è¡Œæ ·æœ¬æ•°æ®
        let request_preview = build_request_preview(&rows_data[0], &field_defs, rows_data.get(1..6).map(|r| r.to_vec()));
        ProcessingEvent {
            event: "ai_request".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            message: Some(request_preview),
            ..Default::default()
        }.emit(app);

        let mapping_result = analyze_columns_with_ai_stream(
            app_clone,
            api_url,
            api_key,
            model_name,
            temperature,
            max_tokens,
            &rows_data[0],
            &field_defs,
            rows_data.get(1..6).map(|r| r.to_vec()),  // åªå–å‰ 5 è¡Œæ ·æœ¬æ•°æ®
            task_id_clone,
            sheet_name_clone,
        ).await?;

        // å‘é€åˆ—æ˜ å°„ç»“æœ
        let mappings_json: HashMap<String, String> = mapping_result.mappings.iter()
            .map(|m| (m.field_name.clone(), m.column_index.to_string()))
            .collect();

        ProcessingEvent {
            event: "column_mapping".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            confidence: Some(mapping_result.confidence),
            mappings: Some(mappings_json.clone()),
            message: Some(format!("åˆ—æ˜ å°„å®Œæˆ (ç½®ä¿¡åº¦: {:.0}%)", mapping_result.confidence * 100.0)),
            ..Default::default()
        }.emit(app);

        // æŒä¹…åŒ–ï¼šæ›´æ–° AI ç½®ä¿¡åº¦å’Œæ˜ å°„æ•°
        let _ = upsert_file_progress(
            db,
            task_id,
            file_name,
            Some(&sheet_name),
            None,  // file_phase
            Some("importing"),
            Some(mapping_result.confidence),
            Some(mapping_result.mappings.len() as i32),
            None,  // success_count
            None,  // error_count
            None,  // total_rows
            None,  // error_message
        ).await;

        // åˆ›å»ºå­—æ®µ ID åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆé¢„ç•™ç”¨äºæœªæ¥ä¼˜åŒ–ï¼‰
        let _field_id_to_idx: HashMap<i32, usize> = fields.iter()
            .enumerate()
            .map(|(i, f)| (f.id, i))
            .collect();

        // å¤„ç†æ•°æ®è¡Œ
        let header_row = mapping_result.header_row.max(0) as usize;
        let start_row = header_row + 1;

        let mut empty_count = 0;

        for (row_idx, row) in rows_data.iter().enumerate().skip(start_row) {
            // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            if control.cancelled.load(Ordering::SeqCst) {
                break;
            }

            // æ£€æŸ¥æš‚åœçŠ¶æ€
            while control.paused.load(Ordering::SeqCst) {
                tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                if control.cancelled.load(Ordering::SeqCst) {
                    return Ok((total_rows, success_count, error_count));
                }
            }

            // ç©ºè¡Œæ£€æµ‹
            let is_empty = row.iter().all(|c| c.trim().is_empty());
            if is_empty {
                empty_count += 1;
                if empty_count >= 10 {
                    break; // è¿ç»­ 10 ä¸ªç©ºè¡Œï¼Œè·³åˆ°ä¸‹ä¸€ä¸ª sheet
                }
                continue;
            }
            empty_count = 0;

            total_rows += 1;

            // æå–æ•°æ®
            let mut data: serde_json::Map<String, serde_json::Value> = serde_json::Map::new();
            let mut validation_errors = Vec::new();

            for mapping in &mapping_result.mappings {
                if let Some(field) = fields.iter().find(|f| f.field_name == mapping.field_name) {
                    let col_idx = mapping.column_index as usize;
                    if col_idx < row.len() {
                        // æ ¹æ®å­—æ®µç±»å‹æ¸…ç†æ•°æ®
                        let value = clean_value(&row[col_idx], &field.field_type);

                        // å¿…å¡«å­—æ®µéªŒè¯
                        if field.is_required && value.trim().is_empty() {
                            validation_errors.push(format!("{} ä¸ºå¿…å¡«é¡¹", field.field_label));
                        }

                        // æ ¼å¼éªŒè¯
                        let rule = field.validation_rule.as_deref();
                        if !validate_value(&value, rule) {
                            validation_errors.push(format!("{} éªŒè¯å¤±è´¥", field.field_label));
                        }

                        // å­˜å‚¨ï¼ˆä½¿ç”¨ field_id ä½œä¸º keyï¼‰
                        data.insert(field.id.to_string(), serde_json::Value::String(value));
                    } else if field.is_required {
                        // åˆ—ä¸å­˜åœ¨ä½†å­—æ®µå¿…å¡«
                        validation_errors.push(format!("{} ä¸ºå¿…å¡«é¡¹", field.field_label));
                    }
                }
            }

            // æ£€æŸ¥å¿…å¡«å­—æ®µæ˜¯å¦åœ¨ AI æ˜ å°„ä¸­å®Œå…¨ç¼ºå¤±ï¼ˆAI æœªèƒ½æ‰¾åˆ°å¯¹åº”åˆ—ï¼‰
            let mapped_field_names: std::collections::HashSet<&str> = mapping_result.mappings
                .iter()
                .map(|m| m.field_name.as_str())
                .collect();
            for field in fields.iter().filter(|f| f.is_required) {
                if !mapped_field_names.contains(field.field_name.as_str()) {
                    validation_errors.push(format!("{} ä¸ºå¿…å¡«é¡¹ï¼ˆæœªæ‰¾åˆ°å¯¹åº”åˆ—ï¼‰", field.field_label));
                }
            }

            // å»é‡æ£€æŸ¥
            let is_duplicate = if dedup_enabled && !dedup_fields.is_empty() {
                let mut dedup_values: HashMap<String, String> = HashMap::new();
                for field_id in dedup_fields {
                    if let Some(val) = data.get(&field_id.to_string()) {
                        if let Some(s) = val.as_str() {
                            dedup_values.insert(field_id.to_string(), s.to_string());
                        }
                    }
                }
                check_duplicate(db, task_id, &dedup_values).await?
            } else {
                false
            };

            // æ’å…¥è®°å½•
            let _status = if validation_errors.is_empty() && !is_duplicate {
                let data_json = serde_json::Value::Object(data);
                insert_record(
                    db,
                    task_id,
                    &data_json,
                    Some(row),  // ä¼ é€’åŸå§‹è¡Œæ•°æ®
                    Some(file_name.to_string()),
                    Some(sheet_name.clone()),
                    Some(row_idx as i32),
                ).await?;
                success_count += 1;
                "success".to_string()
            } else if is_duplicate {
                "duplicate".to_string()
            } else {
                "validation_error".to_string()
            };

            // æ¯ 10 è¡Œå‘é€è¿›åº¦äº‹ä»¶
            if total_rows % 10 == 0 {
                ProcessingEvent {
                    event: "row_processed".to_string(),
                    task_id: task_id.to_string(),
                    current_row: Some(row_idx as i32),
                    total_rows: Some(total_rows),
                    processed_rows: Some(total_rows),
                    success_count: Some(success_count),
                    error_count: Some(error_count),
                    message: Some(format!("å·²å¤„ç† {} è¡Œ", total_rows)),
                    ..Default::default()
                }.emit(app);
            }
        }

        // Sheet å®Œæˆæ—¶è®¡ç®—å½“å‰ Sheet çš„å¢é‡å€¼
        let sheet_success = success_count - sheet_start_success;  // å½“å‰ Sheet çš„æˆåŠŸæ•°ï¼ˆå¢é‡ï¼‰
        let sheet_error = error_count - sheet_start_error;        // å½“å‰ Sheet çš„é”™è¯¯æ•°ï¼ˆå¢é‡ï¼‰
        let sheet_total = total_rows - sheet_start_total;         // å½“å‰ Sheet çš„æ€»è¡Œæ•°ï¼ˆå¢é‡ï¼‰

        // æŒä¹…åŒ–ï¼šæ›´æ–° Sheet å®ŒæˆçŠ¶æ€å’Œç»Ÿè®¡
        let _ = upsert_file_progress(
            db,
            task_id,
            file_name,
            Some(&sheet_name),
            None,  // file_phase
            Some("done"),
            None,  // ai_confidence
            None,  // mapping_count
            Some(sheet_success),
            Some(sheet_error),
            Some(sheet_total),
            None,  // error_message
        ).await;

        // Sheet å®Œæˆ - æ·»åŠ  sheet çº§åˆ«ç»Ÿè®¡å­—æ®µ
        ProcessingEvent {
            event: "sheet_complete".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.to_string()),
            current_sheet: Some(sheet_name.clone()),
            sheet_success_count: Some(sheet_success),
            sheet_error_count: Some(sheet_error),
            sheet_total_rows: Some(sheet_total),
            message: Some(format!("Sheet {} å¤„ç†å®Œæˆ: æˆåŠŸ {} è¡Œ, å¤±è´¥ {} è¡Œ", sheet_name, sheet_success, sheet_error)),
            ..Default::default()
        }.emit(app);
    }

    Ok((total_rows, success_count, error_count))
}

fn build_request_preview(
    headers: &[String],
    field_defs: &[FieldDefinition],
    sample_rows: Option<Vec<Vec<String>>>,
) -> String {
    let mut preview = String::new();
    preview.push_str("ğŸ“¤ å‘é€ç»™ AI çš„æ•°æ®:\n\n");
    preview.push_str("ğŸ“‹ Excel è¡¨å¤´:\n");
    for (i, header) in headers.iter().enumerate() {
        preview.push_str(&format!("  [{}] {}\n", i, header));
    }

    preview.push_str("\nğŸ“ ç›®æ ‡å­—æ®µ:\n");
    for field in field_defs {
        let extra = field.additional_requirement
            .as_ref()
            .map(|r| format!(" ({})", r))
            .unwrap_or_default();
        preview.push_str(&format!(
            "  â€¢ {} [{}]{}: {}\n",
            field.field_name, field.field_type, extra, field.field_label
        ));
    }

    if let Some(rows) = sample_rows {
        preview.push_str("\nğŸ“Š æ ·æœ¬æ•°æ®ï¼ˆåˆ—ç¼–å·ä»1å¼€å§‹ï¼‰:\n");
        for (i, row) in rows.iter().enumerate().take(3) {
            preview.push_str(&format!("  è¡Œ {}: {}\n", i, format_row_indexed(row)));
        }
    }

    preview
}

async fn analyze_columns_with_ai_stream(
    app: AppHandle,
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    headers: &[String],
    field_defs: &[FieldDefinition],
    sample_rows: Option<Vec<Vec<String>>>,
    task_id: String,
    sheet_name: String,
) -> Result<super::ai_service::ColumnMappingResponse, String> {
    let system_prompt = r#"ä½ æ˜¯ä¸“ä¸šçš„ Excel æ•°æ®ç»“æ„åˆ†æä¸“å®¶ï¼Œè´Ÿè´£å°† Excel åˆ—ç²¾å‡†æ˜ å°„åˆ°ç›®æ ‡å­—æ®µã€‚

## æ ¸å¿ƒåŸåˆ™ï¼šä¸¤æ­¥éªŒè¯ï¼ˆç¼ºä¸€ä¸å¯ï¼‰

### ç¬¬ä¸€æ­¥ï¼šåˆ—åè¯­ä¹‰åŒ¹é…
è¡¨å¤´/åˆ—ååœ¨è¯­ä¹‰ä¸Šæ˜¯å¦å¯¹åº”ç›®æ ‡å­—æ®µã€‚

### ç¬¬äºŒæ­¥ï¼šæ•°æ®å†…å®¹éªŒè¯ï¼ˆæœ€é‡è¦ï¼‰
é€åˆ—æ£€æŸ¥æ ·æœ¬æ•°æ®ï¼ŒéªŒè¯å®é™…å†…å®¹æ˜¯å¦ç¬¦åˆå­—æ®µç±»å‹çš„æ•°æ®ç‰¹å¾ï¼š

| å­—æ®µç±»å‹ | æ•°æ®å†…å®¹å¿…é¡»æ»¡è¶³ | å¸¸è§è¯¯åˆ¤é™·é˜± |
|---------|----------------|------------|
| company | å«"æœ‰é™å…¬å¸"ã€"é›†å›¢"ã€Incã€Ltdã€Corp ç­‰æ–‡å­— | âŒ çº¯æ•°å­—/çº¯å­—æ¯ç¼–å·åˆ—åå«"å®¢æˆ·"â†’IDåˆ—ï¼Œä¸æ˜¯å…¬å¸å |
| phone   | 11ä½æ‰‹æœºå·æˆ–å›ºè¯æ ¼å¼ | âŒ å«å­—æ¯çš„ç¼–å·ä¸æ˜¯ç”µè¯ |
| email   | åŒ…å« @ ç¬¦å· | âŒ æ²¡æœ‰@çš„å­—ç¬¦ä¸²ä¸æ˜¯é‚®ç®± |
| name    | 2-4ä¸ªä¸­æ–‡å­—ç¬¦æˆ–è‹±æ–‡äººå | âŒ å«"å…¬å¸"/"é›†å›¢"çš„æ˜¯ä¼ä¸šåä¸æ˜¯å§“å |
| address | å«çœ/å¸‚/åŒº/è·¯/å·/è¡—é“ç­‰ | âŒ çº¯åŸå¸‚åä¸æ˜¯å®Œæ•´åœ°å€ |
| date    | YYYY-MM-DD ç­‰æ—¥æœŸæ ¼å¼ | âŒ çº¯æ•°å­—æ—¶é—´æˆ³ä¸æ˜¯æ—¥æœŸ |
| number  | çº¯æ•°å­—æˆ–å°æ•° | âŒ å«å­—æ¯çš„ç¼–å·ä¸æ˜¯æ•°å­—å­—æ®µ |
| id_card | 15æˆ–18ä½å«å­—æ¯Xçš„èº«ä»½è¯æ ¼å¼ | âŒ æ™®é€š15ä½æ•°å­—ä¸æ˜¯èº«ä»½è¯ |
| url     | ä»¥ http:// æˆ– https:// å¼€å¤´ | âŒ æ²¡æœ‰åè®®å‰ç¼€ä¸æ˜¯URL |
| text    | é€šç”¨æ–‡æœ¬ï¼Œåˆ—åè¯­ä¹‰åŒ¹é…å³å¯ | â€” |

## å†³ç­–è§„åˆ™
- âœ… ä¸¤æ­¥å‡åŒ¹é… â†’ å»ºç«‹æ˜ å°„ï¼Œconfidence åæ˜ ç¡®å®šç¨‹åº¦
- âŒ ä»»æ„ä¸€æ­¥ä¸åŒ¹é… â†’ æ”¾å…¥ unmatched_columnsï¼Œ**å®ç¼ºæ¯‹æ»¥**

## è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰
{
  "header_row": 0,
  "mappings": [
    {"field_name": "å­—æ®µå", "column_index": 0, "column_header": "Excelåˆ—å", "confidence": 0.95}
  ],
  "confidence": 0.9,
  "unmatched_columns": [1, 3]
}

header_row å’Œ column_index å‡ä» 0 è®¡æ•°ï¼›-1 è¡¨ç¤ºæ— è¡¨å¤´"#;

    // åˆ—ç»´åº¦å±•ç¤ºï¼šè¡¨å¤´ + è¯¥åˆ—çš„æ ·æœ¬å€¼ï¼ˆæ–¹ä¾¿ AI é€åˆ—éªŒè¯æ•°æ®å†…å®¹ï¼‰
    let mut user_prompt = String::new();
    user_prompt.push_str("## Excel åˆ—æ•°æ®é¢„è§ˆï¼ˆåˆ—å â†’ æ ·æœ¬å€¼ï¼‰\n\n");
    for (col_idx, header) in headers.iter().enumerate() {
        let col_samples: Vec<&str> = sample_rows
            .as_ref()
            .map(|rows| {
                rows.iter()
                    .filter_map(|row| row.get(col_idx).map(|s| s.as_str()))
                    .filter(|s| !s.trim().is_empty())
                    .take(5)
                    .collect()
            })
            .unwrap_or_default();
        if col_samples.is_empty() {
            user_prompt.push_str(&format!("åˆ—[{}] \"{}\"  â†’  (ç©ºåˆ—)\n", col_idx, header));
        } else {
            user_prompt.push_str(&format!("åˆ—[{}] \"{}\"  â†’  {}\n", col_idx, header, col_samples.join(" | ")));
        }
    }

    // ç›®æ ‡å­—æ®µå®šä¹‰
    user_prompt.push_str("\n## ç›®æ ‡å­—æ®µå®šä¹‰\n\n");
    for field in field_defs {
        let type_rules = get_field_type_rules(&field.field_type);
        let extra = field.additional_requirement
            .as_ref()
            .map(|r| format!("ï¼ˆ{}ï¼‰", r))
            .unwrap_or_default();
        let extraction = field.extraction_hint
            .as_ref()
            .map(|h| format!("\n  æå–è¦æ±‚: {}", h))
            .unwrap_or_default();
        user_prompt.push_str(&format!(
            "- {} [{}]{}: {}\n  æ•°æ®ç‰¹å¾: {}{}\n",
            field.field_name, field.field_type, extra, field.field_label, type_rules, extraction
        ));
    }

    user_prompt.push_str("\n## ä»»åŠ¡\nå¯¹æ¯ä¸€åˆ—æ‰§è¡Œä¸¤æ­¥éªŒè¯ï¼ˆåˆ—åè¯­ä¹‰ + æ•°æ®å†…å®¹ï¼‰ï¼Œè¾“å‡º JSON æ˜ å°„ç»“æœã€‚");


    // ä½¿ç”¨æµå¼è°ƒç”¨ï¼Œæ¯ä¸ª chunk å‘é€äº‹ä»¶
    let app_for_stream = app.clone();
    let task_id_for_stream = task_id.clone();
    let sheet_name_for_stream = sheet_name.clone();

    let response = call_ai_stream(
        api_url,
        api_key,
        model_name,
        system_prompt,
        &user_prompt,
        temperature,
        max_tokens,
        true,  // json_mode: åˆ—æ˜ å°„éœ€è¦è¿”å› JSON
        move |chunk: &str| {
            // å‘é€æµå¼äº‹ä»¶
            let event = ProcessingEvent {
                event: "ai_response".to_string(),
                task_id: task_id_for_stream.clone(),
                current_sheet: Some(sheet_name_for_stream.clone()),
                message: Some(chunk.to_string()),
                ..Default::default()
            };
            event.emit(&app_for_stream);
        },
    ).await?;

    // è§£æå“åº”
    let json_str = extract_json(&response)?;
    let parsed: serde_json::Value = serde_json::from_str(&json_str)
        .map_err(|e| format!("è§£æ JSON å¤±è´¥: {}", e))?;

    let header_row = parsed["header_row"].as_i64().unwrap_or(0) as i32;
    let confidence = parsed["confidence"].as_f64().unwrap_or(0.8) as f32;

    // å†™å…¥ AI è°ƒè¯•æ—¥å¿—ï¼ˆå†™åˆ°ç³»ç»Ÿä¸´æ—¶ç›®å½•ï¼Œé¿å…è§¦å‘ Tauri æ–‡ä»¶ç›‘å¬ï¼‰
    {
        use std::io::Write;
        let log_path = std::env::temp_dir().join("redata_ai_debug.log");
        let mappings_summary = parsed["mappings"]
            .as_array()
            .map(|arr| {
                arr.iter()
                    .map(|m| format!(
                        "  {} -> col[{}] \"{}\" ({:.0}%)",
                        m["field_name"].as_str().unwrap_or("?"),
                        m["column_index"].as_i64().unwrap_or(-1),
                        m["column_header"].as_str().unwrap_or(""),
                        m["confidence"].as_f64().unwrap_or(0.0) * 100.0
                    ))
                    .collect::<Vec<_>>()
                    .join("\n")
            })
            .unwrap_or_default();
        let entry = format!(
            "\n====== AI åˆ—æ˜ å°„æ—¥å¿— [Sheet: {}] ======\n\
            ## è¯·æ±‚\n{}\n\n\
            ## AI åŸå§‹å“åº”\n{}\n\n\
            ## è§£æç»“æœ (header_row={}, confidence={:.0}%)\n{}\n\
            ==========================================\n",
            sheet_name, user_prompt, response, header_row, confidence * 100.0, mappings_summary
        );
        if let Ok(mut f) = std::fs::OpenOptions::new().create(true).append(true).open(&log_path) {
            let _ = f.write_all(entry.as_bytes());
        }
    }

    let mappings: Vec<super::ai_service::FieldMapping> = parsed["mappings"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|m| {
                    Some(super::ai_service::FieldMapping {
                        field_name: m["field_name"].as_str()?.to_string(),
                        column_index: m["column_index"].as_i64()? as i32,
                        column_header: m["column_header"].as_str().unwrap_or("").to_string(),
                        confidence: m["confidence"].as_f64().unwrap_or(0.8) as f32,
                    })
                })
                .collect()
        })
        .unwrap_or_default();

    let unmatched_columns: Vec<i32> = parsed["unmatched_columns"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|v| v.as_i64().map(|i| i as i32))
                .collect()
        })
        .unwrap_or_default();

    Ok(super::ai_service::ColumnMappingResponse {
        header_row,
        mappings,
        confidence,
        unmatched_columns,
    })
}

async fn check_duplicate(db: &Arc<DatabaseConnection>, task_id: &str, dedup_values: &HashMap<String, String>) -> Result<bool, String> {
    // ä» task_id è·å– project_id
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if let Some(task) = task {
        let mut conditions = vec!["project_id = ?".to_string()];
        let mut params: Vec<sea_orm::Value> = vec![task.project_id.into()];

        for (field_id, value) in dedup_values {
            if !value.trim().is_empty() {
                conditions.push(format!("json_extract(data, '$.{}') = ?", field_id));
                params.push(value.clone().into());
            }
        }

        if conditions.len() > 1 {
            let sql = format!(
                "SELECT id FROM project_records WHERE {} LIMIT 1",
                conditions.join(" AND ")
            );

            let result = db.as_ref()
                .query_one(Statement::from_sql_and_values(
                    db.as_ref().get_database_backend(),
                    &sql,
                    params,
                ))
                .await
                .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

            return Ok(result.is_some());
        }
    }

    Ok(false)
}

async fn insert_record(
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    data: &serde_json::Value,
    raw_data: Option<&[String]>,
    source_file: Option<String>,
    source_sheet: Option<String>,
    row_number: Option<i32>,
) -> Result<i32, String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let now = chrono::Utc::now().to_rfc3339();
    let data_str = serde_json::to_string(data)
        .map_err(|e| format!("JSON åºåˆ—åŒ–é”™è¯¯: {}", e))?;

    // åºåˆ—åŒ–åŸå§‹è¡Œæ•°æ®ä¸ºç´¢å¼•æ ¼å¼ï¼š1:åˆ—1å†…å®¹;2:åˆ—2å†…å®¹;...n:åˆ—nå†…å®¹;
    let raw_data_str = raw_data.map(|row| format_row_indexed(row));

    let new_record = record::ActiveModel {
        project_id: Set(task.project_id),
        data: Set(data_str),
        raw_data: Set(raw_data_str),
        source_file: Set(source_file),
        source_sheet: Set(source_sheet),
        row_number: Set(row_number),
        batch_number: Set(task.batch_number.clone()),
        status: Set("success".to_string()),
        error_message: Set(None),
        created_at: Set(now),
        updated_at: Set(None),
        ..Default::default()
    };

    let result = new_record
        .insert(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(result.id)
}

async fn update_task_status(db: &Arc<DatabaseConnection>, task_id: &str, status: String) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let mut active: task::ActiveModel = task.into();
    active.status = Set(status);
    active.updated_at = Set(Some(chrono::Utc::now()));

    active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(())
}

async fn update_task_progress(
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    processed_files: i32,
    total_rows: i32,
    processed_rows: i32,
    success_count: i32,
    error_count: i32,
) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let mut active: task::ActiveModel = task.into();
    active.processed_files = Set(processed_files);
    active.total_rows = Set(total_rows);
    active.processed_rows = Set(processed_rows);
    active.success_count = Set(success_count);
    active.error_count = Set(error_count);
    active.updated_at = Set(Some(chrono::Utc::now()));

    active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(())
}

/// æš‚åœä»»åŠ¡
#[tauri::command]
pub async fn pause_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.paused.store(true, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "paused".to_string()).await
}

/// æ¢å¤ä»»åŠ¡
#[tauri::command]
pub async fn resume_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.paused.store(false, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "processing".to_string()).await
}

/// å–æ¶ˆä»»åŠ¡
#[tauri::command]
pub async fn cancel_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.cancelled.store(true, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "cancelled".to_string()).await
}
