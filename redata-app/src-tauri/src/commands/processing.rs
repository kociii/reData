// æ•°æ®å¤„ç† Tauri Commands
//
// æ ¸å¿ƒå¤„ç†æµç¨‹ï¼šAI åˆ—æ˜ å°„ + æœ¬åœ°éªŒè¯å¯¼å…¥
// ä½¿ç”¨ Tauri äº‹ä»¶ç³»ç»Ÿæ¨é€è¿›åº¦

use calamine::{open_workbook_auto, Reader, Data};
use regex::Regex;
use sea_orm::{
    ActiveModelTrait, ColumnTrait, ConnectionTrait, DatabaseConnection,
    EntityTrait, QueryFilter, QueryOrder, Set, Statement,
};
use serde::Serialize;
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, LazyLock};
use tauri::{AppHandle, Emitter};
use tokio::sync::RwLock;

use crate::backend::infrastructure::{
    config::decrypt,
    persistence::models::{
        task, ProcessingTask, field,
        AiConfig as AiConfigModel, Project, record,
    },
};
use field::Model as FieldModel;
use super::ai_utils::{call_ai_stream, extract_json};
use super::ai_service::FieldDefinition;

// ============ ä»»åŠ¡æ§åˆ¶ ============

struct TaskControl {
    paused: AtomicBool,
    cancelled: AtomicBool,
}

static ACTIVE_TASKS: LazyLock<RwLock<HashMap<String, Arc<TaskControl>>>> =
    LazyLock::new(|| RwLock::new(HashMap::new()));

// ============ äº‹ä»¶ç»“æ„ ============

#[derive(Debug, Clone, Serialize, Default)]
pub struct ProcessingEvent {
    pub event: String,
    pub task_id: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_file: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_sheet: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub current_row: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub total_rows: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub processed_rows: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub success_count: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error_count: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub confidence: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub mappings: Option<HashMap<String, String>>,
}

impl ProcessingEvent {
    fn emit(&self, app: &AppHandle) {
        let _ = app.emit("processing-progress", self);
    }
}

// ============ å“åº”ç»“æ„ ============

#[derive(Debug, Serialize)]
pub struct StartProcessingResponse {
    pub task_id: String,
    pub batch_number: String,
    pub project_id: i32,
    pub status: String,
    pub source_files: Vec<String>,
}

// ============ è¾…åŠ©å‡½æ•° ============

fn data_to_string(data: &Data) -> String {
    match data {
        Data::Int(i) => i.to_string(),
        Data::Float(f) => {
            if *f == (*f as i64) as f64 {
                (*f as i64).to_string()
            } else {
                f.to_string()
            }
        }
        Data::String(s) => s.clone(),
        Data::Bool(b) => b.to_string(),
        Data::DateTime(dt) => dt.to_string(),
        Data::DateTimeIso(s) => s.clone(),
        Data::DurationIso(s) => s.clone(),
        Data::Error(e) => format!("#ERR:{:?}", e),
        Data::Empty => String::new(),
    }
}

fn validate_value(value: &str, validation_rule: Option<&str>) -> bool {
    if value.trim().is_empty() {
        return true; // ç©ºå€¼é€šè¿‡ï¼ˆç”± required å­—æ®µå¤„ç†ï¼‰
    }
    if let Some(rule) = validation_rule {
        if let Ok(re) = Regex::new(rule) {
            return re.is_match(value);
        }
    }
    true
}

/// æ ¹æ®å­—æ®µç±»å‹æ¸…ç†æ•°æ®å€¼
///
/// æ¸…ç†è§„åˆ™ï¼š
/// - é€šç”¨ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦
/// - phone: ä»…ä¿ç•™æ•°å­—å’Œ + å·
/// - email: å»é™¤ç©ºæ ¼ã€æ¢è¡Œï¼Œè½¬å°å†™
/// - text/å…¶ä»–: å‹ç¼©è¿ç»­ç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
fn clean_value(value: &str, field_type: &str) -> String {
    // ç¬¬ä¸€æ­¥ï¼šé€šç”¨æ¸…ç† - å»é™¤é¦–å°¾ç©ºç™½å’Œæ§åˆ¶å­—ç¬¦
    let mut cleaned = value
        .chars()
        .map(|c| match c {
            '\r' | '\n' | '\t' => ' ',  // æ¢è¡Œã€åˆ¶è¡¨ç¬¦è½¬ä¸ºç©ºæ ¼
            c if c.is_control() => ' ', // å…¶ä»–æ§åˆ¶å­—ç¬¦è½¬ä¸ºç©ºæ ¼
            c => c,
        })
        .collect::<String>();

    // æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œç‰¹å®šæ¸…ç†
    match field_type {
        "phone" => {
            // æ‰‹æœºå·ï¼šä»…ä¿ç•™æ•°å­—å’Œ + å·
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_digit() || *c == '+')
                .collect();
        }
        "email" => {
            // é‚®ç®±ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼ï¼Œè½¬å°å†™
            cleaned = cleaned.chars().filter(|c| !c.is_whitespace()).collect();
            cleaned = cleaned.to_lowercase();
        }
        "number" | "id_card" => {
            // æ•°å­—ã€èº«ä»½è¯ï¼šä»…ä¿ç•™æ•°å­—å’Œå­—æ¯
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_alphanumeric())
                .collect();
        }
        "date" => {
            // æ—¥æœŸï¼šå»é™¤ç©ºæ ¼ï¼Œä¿ç•™æ•°å­—ã€æ—¥æœŸåˆ†éš”ç¬¦
            cleaned = cleaned
                .chars()
                .filter(|c| c.is_ascii_digit() || matches!(c, '-' | '/' | '.' | ':'))
                .collect();
        }
        _ => {
            // é»˜è®¤æ–‡æœ¬ç±»å‹ï¼šå‹ç¼©è¿ç»­ç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
            let mut result = String::new();
            let mut prev_space = false;
            for c in cleaned.chars() {
                if c.is_whitespace() {
                    if !prev_space {
                        result.push(' ');
                        prev_space = true;
                    }
                } else {
                    result.push(c);
                    prev_space = false;
                }
            }
            cleaned = result;
        }
    }

    // æœ€åå†æ¬¡ trim
    cleaned.trim().to_string()
}

// ============ Tauri Commands ============

/// å¼€å§‹å¤„ç†æ–‡ä»¶
#[tauri::command]
pub async fn start_processing(
    app: AppHandle,
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    project_id: i32,
    file_paths: Vec<String>,
    ai_config_id: Option<i32>,
) -> Result<StartProcessingResponse, String> {
    // è·å–æ•°æ®åº“è¿æ¥çš„å…‹éš†
    let db_conn = db.inner().clone();

    // 1. éªŒè¯é¡¹ç›®
    let project = Project::find_by_id(project_id)
        .one(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("é¡¹ç›® {} ä¸å­˜åœ¨", project_id))?;

    // 2. è·å–å­—æ®µå®šä¹‰
    let fields = field::Entity::find()
        .filter(field::Column::ProjectId.eq(project_id))
        .filter(field::Column::IsDeleted.eq(false))
        .order_by(field::Column::DisplayOrder, sea_orm::Order::Asc)
        .all(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if fields.is_empty() {
        return Err("é¡¹ç›®æ²¡æœ‰å®šä¹‰å­—æ®µ".to_string());
    }

    // 3. è·å– AI é…ç½®
    let ai_config = if let Some(config_id) = ai_config_id {
        AiConfigModel::find_by_id(config_id)
            .one(db_conn.as_ref())
            .await
            .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
            .ok_or_else(|| format!("AI é…ç½® {} ä¸å­˜åœ¨", config_id))?
    } else {
        // ä½¿ç”¨é»˜è®¤é…ç½®
        let configs = AiConfigModel::find()
            .all(db_conn.as_ref())
            .await
            .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
        configs.into_iter()
            .find(|c| c.is_default)
            .ok_or_else(|| "æ²¡æœ‰é»˜è®¤ AI é…ç½®".to_string())?
    };

    // 4. è§£å¯† API Key
    let api_key = decrypt(&ai_config.api_key)
        .map_err(|e| format!("è§£å¯†å¤±è´¥: {}", e))?;

    // 5. åˆ›å»ºä»»åŠ¡
    let task_id = uuid::Uuid::new_v4().to_string();
    let now = chrono::Utc::now();

    // ç”Ÿæˆ batch_number
    let date_str = now.format("%Y%m%d").to_string();
    let count = ProcessingTask::find()
        .filter(task::Column::ProjectId.eq(project_id))
        .filter(task::Column::BatchNumber.starts_with(&format!("BATCH_{}", date_str)))
        .all(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .len();
    let batch_number = format!("BATCH_{}_{:03}", date_str, count + 1);

    // åˆ›å»ºä»»åŠ¡è®°å½•
    // æå–æºæ–‡ä»¶ååˆ—è¡¨
    let source_file_names: Vec<String> = file_paths
        .iter()
        .map(|p| {
            std::path::Path::new(p)
                .file_name()
                .map(|s| s.to_string_lossy().to_string())
                .unwrap_or_else(|| "unknown".to_string())
        })
        .collect();
    let source_files_json = serde_json::to_string(&source_file_names)
        .unwrap_or_else(|_| "[]".to_string());

    let new_task = task::ActiveModel {
        id: Set(task_id.clone()),
        project_id: Set(project_id),
        status: Set("processing".to_string()),
        total_files: Set(file_paths.len() as i32),
        processed_files: Set(0),
        total_rows: Set(0),
        processed_rows: Set(0),
        success_count: Set(0),
        error_count: Set(0),
        batch_number: Set(Some(batch_number.clone())),
        source_files: Set(Some(source_files_json)),
        created_at: Set(now),
        updated_at: Set(None),
    };

    new_task
        .insert(db_conn.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    // 6. æ³¨å†Œä»»åŠ¡æ§åˆ¶
    let control = Arc::new(TaskControl {
        paused: AtomicBool::new(false),
        cancelled: AtomicBool::new(false),
    });
    {
        let mut tasks = ACTIVE_TASKS.write().await;
        tasks.insert(task_id.clone(), control.clone());
    }

    // 7. å¯åŠ¨åå°å¤„ç†
    let app_for_spawn = app.clone();
    let project_clone = project.clone();
    let fields_clone = fields.clone();
    let api_url = ai_config.api_url.clone();
    let api_key_clone = api_key.clone();
    let model_name = ai_config.model_name.clone();
    let temperature = ai_config.temperature;
    let max_tokens = ai_config.max_tokens;
    let task_id_clone = task_id.clone();

    tokio::spawn(async move {
        let task_id_inner = task_id_clone;
        let result = process_files(
            app_for_spawn,
            db_conn.clone(),
            &task_id_inner,
            &project_clone,
            &fields_clone,
            &file_paths,
            &api_url,
            &api_key_clone,
            &model_name,
            temperature,
            max_tokens,
            control.clone(),
        ).await;

        // æ¸…ç†ä»»åŠ¡æ§åˆ¶
        {
            let mut tasks = ACTIVE_TASKS.write().await;
            tasks.remove(&task_id_inner);
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        if let Err(e) = result {
            let _ = update_task_error(&db_conn.clone(), &task_id_inner, &e).await;
            let event = ProcessingEvent {
                event: "error".to_string(),
                task_id: task_id_inner.clone(),
                message: Some(e),
                ..Default::default()
            };
            event.emit(&app);
        }
    });

    Ok(StartProcessingResponse {
        task_id,
        batch_number,
        project_id,
        status: "processing".to_string(),
        source_files: source_file_names,
    })
}

async fn update_task_error(db: &Arc<DatabaseConnection>, task_id: &str, _error: &str) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if let Some(task) = task {
        let mut active: task::ActiveModel = task.into();
        active.status = Set("error".to_string());
        active.updated_at = Set(Some(chrono::Utc::now()));
        active.error_count = Set(1);
        active.updated_at = Set(Some(chrono::Utc::now()));
        active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
    }
    Ok(())
}

async fn process_files(
    app: AppHandle,
    db: Arc<DatabaseConnection>,
    task_id: &str,
    project: &crate::backend::infrastructure::persistence::models::project::Model,
    fields: &[FieldModel],
    file_paths: &[String],
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    control: Arc<TaskControl>,
) -> Result<(), String> {
    let mut total_rows = 0i32;
    let mut processed_rows = 0i32;
    let mut success_count = 0i32;
    let mut error_count = 0i32;

    // è·å–å»é‡å­—æ®µ
    let dedup_fields: Vec<i32> = if project.dedup_enabled {
        fields.iter()
            .filter(|f| f.is_dedup_key)
            .map(|f| f.id)
            .collect()
    } else {
        vec![]
    };

    for (file_idx, file_path) in file_paths.iter().enumerate() {
        // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if control.cancelled.load(Ordering::SeqCst) {
            update_task_status(&db, task_id, "cancelled".to_string()).await?;
            return Ok(());
        }

        // ç­‰å¾…æ¢å¤
        while control.paused.load(Ordering::SeqCst) {
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            if control.cancelled.load(Ordering::SeqCst) {
                update_task_status(&db, task_id, "cancelled".to_string()).await?;
                return Ok(());
            }
        }

        let file_name = std::path::Path::new(file_path)
            .file_name()
            .map(|s| s.to_string_lossy().to_string())
            .unwrap_or_else(|| "unknown".to_string());

        // å‘é€æ–‡ä»¶å¼€å§‹äº‹ä»¶
        ProcessingEvent {
            event: "file_start".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.clone()),
            message: Some(format!("å¼€å§‹å¤„ç†æ–‡ä»¶: {}", file_name)),
            ..Default::default()
        }.emit(&app);

        // å¤„ç†æ–‡ä»¶
        let result = process_single_file(
            &app,
            &db,
            task_id,
            file_path.clone(),
            &file_name,
            fields,
            api_url,
            api_key,
            model_name,
            temperature,
            max_tokens,
            &dedup_fields,
            project.dedup_enabled,
            &control,
        ).await;

        match result {
            Ok((rows, success, errors)) => {
                total_rows += rows;
                processed_rows += rows;
                success_count += success;
                error_count += errors;
            }
            Err(e) => {
                error_count += 1;
                ProcessingEvent {
                    event: "error".to_string(),
                    task_id: task_id.to_string(),
                    current_file: Some(file_name.clone()),
                    message: Some(e),
                    ..Default::default()
                }.emit(&app);
            }
        }

        // æ›´æ–°ä»»åŠ¡è¿›åº¦
        update_task_progress(&db, task_id, (file_idx + 1) as i32, total_rows, processed_rows, success_count, error_count).await?;

        // å‘é€æ–‡ä»¶å®Œæˆäº‹ä»¶
        ProcessingEvent {
            event: "file_complete".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.clone()),
            processed_rows: Some(processed_rows),
            success_count: Some(success_count),
            error_count: Some(error_count),
            message: Some(format!("æ–‡ä»¶å¤„ç†å®Œæˆ: {} è¡Œ", processed_rows)),
            ..Default::default()
        }.emit(&app);
    }

    // æ›´æ–°ä»»åŠ¡ä¸ºå®Œæˆ
    update_task_status(&db, task_id, "completed".to_string()).await?;

    // å‘é€å®Œæˆäº‹ä»¶
    ProcessingEvent {
        event: "completed".to_string(),
        task_id: task_id.to_string(),
        processed_rows: Some(processed_rows),
        success_count: Some(success_count),
        error_count: Some(error_count),
        message: Some(format!("å¤„ç†å®Œæˆ: æˆåŠŸ {} è¡Œ, å¤±è´¥ {} è¡Œ", success_count, error_count)),
        ..Default::default()
    }.emit(&app);

    Ok(())
}

async fn process_single_file(
    app: &AppHandle,
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    file_path: String,
    file_name: &str,
    fields: &[FieldModel],
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    dedup_fields: &[i32],
    dedup_enabled: bool,
    control: &Arc<TaskControl>,
) -> Result<(i32, i32, i32), String> {
    let mut total_rows = 0i32;
    let mut success_count = 0i32;
    let error_count = 0i32;

    // ä½¿ç”¨ spawn_blocking è¯»å– Excel å¹¶å¤„ç†æ‰€æœ‰ sheets
    let result = tokio::task::spawn_blocking(move || {
        let mut workbook = open_workbook_auto(file_path)
            .map_err(|e| format!("æ— æ³•æ‰“å¼€æ–‡ä»¶: {}", e))?;
        let sheet_names = workbook.sheet_names().to_vec();
        let mut all_rows: HashMap<String, Vec<Vec<String>>> = HashMap::new();

        for sheet_name in &sheet_names {
            let range = workbook.worksheet_range(sheet_name)
                .map_err(|e| format!("æ— æ³•è¯»å– Sheet: {}", e))?;
            let rows: Vec<Vec<String>> = range
                .rows()
                .map(|row| row.iter().map(data_to_string).collect())
                .collect();
            all_rows.insert(sheet_name.clone(), rows);
        }

        Ok::<_, String>((sheet_names, all_rows))
    })
    .await
    .map_err(|e| format!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e))?
    .map_err(|e| format!("æ— æ³•æ‰“å¼€æ–‡ä»¶: {}", e))?;

    let (sheet_names, mut all_rows) = result;

    for sheet_name in sheet_names {
        // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if control.cancelled.load(Ordering::SeqCst) {
            return Ok((total_rows, success_count, error_count));
        }

        // å‘é€ Sheet å¼€å§‹äº‹ä»¶
        ProcessingEvent {
            event: "sheet_start".to_string(),
            task_id: task_id.to_string(),
            current_file: Some(file_name.to_string()),
            current_sheet: Some(sheet_name.clone()),
            message: Some(format!("å¼€å§‹å¤„ç† Sheet: {}", sheet_name)),
            ..Default::default()
        }.emit(app);

        // è·å–å·²è¯»å–çš„ Sheet æ•°æ®
        let rows_data = match all_rows.remove(&sheet_name) {
            Some(rows) => rows,
            None => continue,
        };

        if rows_data.is_empty() {
            continue;
        }

        // AI åˆ†æåˆ—æ˜ å°„
        ProcessingEvent {
            event: "ai_analyzing".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            message: Some("AI åˆ†æåˆ—æ˜ å°„...".to_string()),
            ..Default::default()
        }.emit(app);

        // æ„å»ºå­—æ®µå®šä¹‰
        let field_defs: Vec<FieldDefinition> = fields.iter().map(|f| FieldDefinition {
            field_name: f.field_name.clone(),
            field_label: f.field_label.clone(),
            field_type: f.field_type.clone(),
            additional_requirement: f.additional_requirement.clone(),
        }).collect();

        // AI åˆ†æï¼ˆæµå¼ï¼‰
        let app_clone = app.clone();
        let task_id_clone = task_id.to_string();
        let sheet_name_clone = sheet_name.clone();

        // æ„å»ºè¯·æ±‚æç¤ºï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        let request_preview = build_request_preview(&rows_data[0], &field_defs, rows_data.get(1..11).map(|r| r.to_vec()));
        ProcessingEvent {
            event: "ai_request".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            message: Some(request_preview),
            ..Default::default()
        }.emit(app);

        let mapping_result = analyze_columns_with_ai_stream(
            app_clone,
            api_url,
            api_key,
            model_name,
            temperature,
            max_tokens,
            &rows_data[0],
            &field_defs,
            rows_data.get(1..11).map(|r| r.to_vec()),
            task_id_clone,
            sheet_name_clone,
        ).await?;

        // å‘é€åˆ—æ˜ å°„ç»“æœ
        let mappings_json: HashMap<String, String> = mapping_result.mappings.iter()
            .map(|m| (m.field_name.clone(), m.column_index.to_string()))
            .collect();

        ProcessingEvent {
            event: "column_mapping".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            confidence: Some(mapping_result.confidence),
            mappings: Some(mappings_json.clone()),
            message: Some(format!("åˆ—æ˜ å°„å®Œæˆ (ç½®ä¿¡åº¦: {:.0}%)", mapping_result.confidence * 100.0)),
            ..Default::default()
        }.emit(app);

        // åˆ›å»ºå­—æ®µ ID åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆé¢„ç•™ç”¨äºæœªæ¥ä¼˜åŒ–ï¼‰
        let _field_id_to_idx: HashMap<i32, usize> = fields.iter()
            .enumerate()
            .map(|(i, f)| (f.id, i))
            .collect();

        // å¤„ç†æ•°æ®è¡Œ
        let header_row = mapping_result.header_row.max(0) as usize;
        let start_row = header_row + 1;

        let mut empty_count = 0;

        for (row_idx, row) in rows_data.iter().enumerate().skip(start_row) {
            // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            if control.cancelled.load(Ordering::SeqCst) {
                break;
            }

            // æ£€æŸ¥æš‚åœçŠ¶æ€
            while control.paused.load(Ordering::SeqCst) {
                tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                if control.cancelled.load(Ordering::SeqCst) {
                    return Ok((total_rows, success_count, error_count));
                }
            }

            // ç©ºè¡Œæ£€æµ‹
            let is_empty = row.iter().all(|c| c.trim().is_empty());
            if is_empty {
                empty_count += 1;
                if empty_count >= 10 {
                    break; // è¿ç»­ 10 ä¸ªç©ºè¡Œï¼Œè·³åˆ°ä¸‹ä¸€ä¸ª sheet
                }
                continue;
            }
            empty_count = 0;

            total_rows += 1;

            // æå–æ•°æ®
            let mut data: serde_json::Map<String, serde_json::Value> = serde_json::Map::new();
            let mut validation_errors = Vec::new();

            for mapping in &mapping_result.mappings {
                if let Some(field) = fields.iter().find(|f| f.field_name == mapping.field_name) {
                    let col_idx = mapping.column_index as usize;
                    if col_idx < row.len() {
                        // æ ¹æ®å­—æ®µç±»å‹æ¸…ç†æ•°æ®
                        let value = clean_value(&row[col_idx], &field.field_type);

                        // éªŒè¯
                        let rule = field.validation_rule.as_deref();
                        if !validate_value(&value, rule) {
                            validation_errors.push(format!("{} éªŒè¯å¤±è´¥", field.field_label));
                        }

                        // å­˜å‚¨ï¼ˆä½¿ç”¨ field_id ä½œä¸º keyï¼‰
                        data.insert(field.id.to_string(), serde_json::Value::String(value));
                    }
                }
            }

            // å»é‡æ£€æŸ¥
            let is_duplicate = if dedup_enabled && !dedup_fields.is_empty() {
                let mut dedup_values: HashMap<String, String> = HashMap::new();
                for field_id in dedup_fields {
                    if let Some(val) = data.get(&field_id.to_string()) {
                        if let Some(s) = val.as_str() {
                            dedup_values.insert(field_id.to_string(), s.to_string());
                        }
                    }
                }
                check_duplicate(db, task_id, &dedup_values).await?
            } else {
                false
            };

            // æ’å…¥è®°å½•
            let _status = if validation_errors.is_empty() && !is_duplicate {
                let data_json = serde_json::Value::Object(data);
                insert_record(
                    db,
                    task_id,
                    &data_json,
                    Some(row),  // ä¼ é€’åŸå§‹è¡Œæ•°æ®
                    Some(file_name.to_string()),
                    Some(sheet_name.clone()),
                    Some(row_idx as i32),
                ).await?;
                success_count += 1;
                "success".to_string()
            } else if is_duplicate {
                "duplicate".to_string()
            } else {
                "validation_error".to_string()
            };

            // æ¯ 10 è¡Œå‘é€è¿›åº¦äº‹ä»¶
            if total_rows % 10 == 0 {
                ProcessingEvent {
                    event: "row_processed".to_string(),
                    task_id: task_id.to_string(),
                    current_row: Some(row_idx as i32),
                    total_rows: Some(total_rows),
                    processed_rows: Some(total_rows),
                    success_count: Some(success_count),
                    error_count: Some(error_count),
                    message: Some(format!("å·²å¤„ç† {} è¡Œ", total_rows)),
                    ..Default::default()
                }.emit(app);
            }
        }

        // Sheet å®Œæˆ
        ProcessingEvent {
            event: "sheet_complete".to_string(),
            task_id: task_id.to_string(),
            current_sheet: Some(sheet_name.clone()),
            message: Some(format!("Sheet {} å¤„ç†å®Œæˆ", sheet_name)),
            ..Default::default()
        }.emit(app);
    }

    Ok((total_rows, success_count, error_count))
}

fn build_request_preview(
    headers: &[String],
    field_defs: &[FieldDefinition],
    sample_rows: Option<Vec<Vec<String>>>,
) -> String {
    let mut preview = String::new();
    preview.push_str("ğŸ“¤ å‘é€ç»™ AI çš„æ•°æ®:\n\n");
    preview.push_str("ğŸ“‹ Excel è¡¨å¤´:\n");
    for (i, header) in headers.iter().enumerate() {
        preview.push_str(&format!("  [{}] {}\n", i, header));
    }

    preview.push_str("\nğŸ“ ç›®æ ‡å­—æ®µ:\n");
    for field in field_defs {
        let extra = field.additional_requirement
            .as_ref()
            .map(|r| format!(" ({})", r))
            .unwrap_or_default();
        preview.push_str(&format!(
            "  â€¢ {} [{}]{}: {}\n",
            field.field_name, field.field_type, extra, field.field_label
        ));
    }

    if let Some(rows) = sample_rows {
        preview.push_str("\nğŸ“Š æ ·æœ¬æ•°æ®:\n");
        for (i, row) in rows.iter().enumerate().take(3) {
            let preview_row: Vec<&str> = row.iter().take(5).map(|s| s.as_str()).collect();
            preview.push_str(&format!("  è¡Œ {}: {}\n", i, preview_row.join(" | ")));
            if row.len() > 5 {
                preview.push_str(&format!("       ... (å…± {} åˆ—)\n", row.len()));
            }
        }
    }

    preview
}

async fn analyze_columns_with_ai_stream(
    app: AppHandle,
    api_url: &str,
    api_key: &str,
    model_name: &str,
    temperature: f32,
    max_tokens: i32,
    headers: &[String],
    field_defs: &[FieldDefinition],
    sample_rows: Option<Vec<Vec<String>>>,
    task_id: String,
    sheet_name: String,
) -> Result<super::ai_service::ColumnMappingResponse, String> {
    let system_prompt = r#"ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¤„ç†ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æ Excel è¡¨æ ¼çš„åˆ—ä¸ç›®æ ‡å­—æ®µçš„æ˜ å°„å…³ç³»ã€‚

ä»»åŠ¡ï¼š
1. è¯†åˆ«è¡¨å¤´æ‰€åœ¨è¡Œï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡ŒåŒ…å«å­—æ®µåçš„è¡Œï¼‰
2. åˆ†ææ¯ä¸€åˆ—ä¸ç›®æ ‡å­—æ®µçš„åŒ¹é…å…³ç³»
3. è¿”å› JSON æ ¼å¼çš„æ˜ å°„ç»“æœ

è¿”å›æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ï¼š
{
  "header_row": 0,
  "mappings": [
    {"field_name": "ç›®æ ‡å­—æ®µå", "column_index": 0, "column_header": "Excelè¡¨å¤´", "confidence": 0.95}
  ],
  "confidence": 0.9,
  "unmatched_columns": []
}

æ³¨æ„ï¼š
- header_row ä» 0 å¼€å§‹è®¡æ•°ï¼Œ-1 è¡¨ç¤ºæ²¡æœ‰è¡¨å¤´
- column_index ä» 0 å¼€å§‹
- confidence èŒƒå›´ 0-1ï¼Œè¡¨ç¤ºåŒ¹é…ç½®ä¿¡åº¦
- å¦‚æœæŸåˆ—æ— æ³•åŒ¹é…ä»»ä½•ç›®æ ‡å­—æ®µï¼Œæ”¾å…¥ unmatched_columns"#;

    let mut user_prompt = String::new();
    user_prompt.push_str("Excel è¡¨å¤´ï¼ˆæŒ‰é¡ºåºï¼‰ï¼š\n");
    for (i, header) in headers.iter().enumerate() {
        user_prompt.push_str(&format!("  [{}] {}\n", i, header));
    }

    user_prompt.push_str("\nç›®æ ‡å­—æ®µå®šä¹‰ï¼š\n");
    for field in field_defs {
        let extra = field.additional_requirement
            .as_ref()
            .map(|r| format!(" ({})", r))
            .unwrap_or_default();
        user_prompt.push_str(&format!(
            "  - {} [{}]{}: {}\n",
            field.field_name, field.field_type, extra, field.field_label
        ));
    }

    if let Some(rows) = sample_rows {
        user_prompt.push_str("\næ ·æœ¬æ•°æ®ï¼ˆå‰å‡ è¡Œï¼‰ï¼š\n");
        for (i, row) in rows.iter().enumerate() {
            user_prompt.push_str(&format!("  è¡Œ {}: {}\n", i, row.join(" | ")));
        }
    }

    user_prompt.push_str("\nè¯·åˆ†æåˆ—æ˜ å°„å…³ç³»å¹¶è¿”å› JSON ç»“æœã€‚");

    // ä½¿ç”¨æµå¼è°ƒç”¨ï¼Œæ¯ä¸ª chunk å‘é€äº‹ä»¶
    let app_for_stream = app.clone();
    let task_id_for_stream = task_id.clone();
    let sheet_name_for_stream = sheet_name.clone();

    let response = call_ai_stream(
        api_url,
        api_key,
        model_name,
        system_prompt,
        &user_prompt,
        temperature,
        max_tokens,
        true,  // json_mode: åˆ—æ˜ å°„éœ€è¦è¿”å› JSON
        move |chunk: &str| {
            // å‘é€æµå¼äº‹ä»¶
            let event = ProcessingEvent {
                event: "ai_response".to_string(),
                task_id: task_id_for_stream.clone(),
                current_sheet: Some(sheet_name_for_stream.clone()),
                message: Some(chunk.to_string()),
                ..Default::default()
            };
            event.emit(&app_for_stream);
        },
    ).await?;

    // è§£æå“åº”
    let json_str = extract_json(&response)?;
    let parsed: serde_json::Value = serde_json::from_str(&json_str)
        .map_err(|e| format!("è§£æ JSON å¤±è´¥: {}", e))?;

    let header_row = parsed["header_row"].as_i64().unwrap_or(0) as i32;
    let confidence = parsed["confidence"].as_f64().unwrap_or(0.8) as f32;

    let mappings: Vec<super::ai_service::FieldMapping> = parsed["mappings"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|m| {
                    Some(super::ai_service::FieldMapping {
                        field_name: m["field_name"].as_str()?.to_string(),
                        column_index: m["column_index"].as_i64()? as i32,
                        column_header: m["column_header"].as_str().unwrap_or("").to_string(),
                        confidence: m["confidence"].as_f64().unwrap_or(0.8) as f32,
                    })
                })
                .collect()
        })
        .unwrap_or_default();

    let unmatched_columns: Vec<i32> = parsed["unmatched_columns"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|v| v.as_i64().map(|i| i as i32))
                .collect()
        })
        .unwrap_or_default();

    Ok(super::ai_service::ColumnMappingResponse {
        header_row,
        mappings,
        confidence,
        unmatched_columns,
    })
}

async fn check_duplicate(db: &Arc<DatabaseConnection>, task_id: &str, dedup_values: &HashMap<String, String>) -> Result<bool, String> {
    // ä» task_id è·å– project_id
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    if let Some(task) = task {
        let mut conditions = vec!["project_id = ?".to_string()];
        let mut params: Vec<sea_orm::Value> = vec![task.project_id.into()];

        for (field_id, value) in dedup_values {
            if !value.trim().is_empty() {
                conditions.push(format!("json_extract(data, '$.{}') = ?", field_id));
                params.push(value.clone().into());
            }
        }

        if conditions.len() > 1 {
            let sql = format!(
                "SELECT id FROM project_records WHERE {} LIMIT 1",
                conditions.join(" AND ")
            );

            let result = db.as_ref()
                .query_one(Statement::from_sql_and_values(
                    db.as_ref().get_database_backend(),
                    &sql,
                    params,
                ))
                .await
                .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

            return Ok(result.is_some());
        }
    }

    Ok(false)
}

async fn insert_record(
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    data: &serde_json::Value,
    raw_data: Option<&[String]>,
    source_file: Option<String>,
    source_sheet: Option<String>,
    row_number: Option<i32>,
) -> Result<i32, String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let now = chrono::Utc::now().to_rfc3339();
    let data_str = serde_json::to_string(data)
        .map_err(|e| format!("JSON åºåˆ—åŒ–é”™è¯¯: {}", e))?;

    // åºåˆ—åŒ–åŸå§‹è¡Œæ•°æ®
    let raw_data_str = raw_data.map(|row| {
        serde_json::to_string(row).unwrap_or_else(|_| "[]".to_string())
    });

    let new_record = record::ActiveModel {
        project_id: Set(task.project_id),
        data: Set(data_str),
        raw_data: Set(raw_data_str),
        source_file: Set(source_file),
        source_sheet: Set(source_sheet),
        row_number: Set(row_number),
        batch_number: Set(task.batch_number.clone()),
        status: Set("success".to_string()),
        error_message: Set(None),
        created_at: Set(now),
        updated_at: Set(None),
        ..Default::default()
    };

    let result = new_record
        .insert(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(result.id)
}

async fn update_task_status(db: &Arc<DatabaseConnection>, task_id: &str, status: String) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let mut active: task::ActiveModel = task.into();
    active.status = Set(status);
    active.updated_at = Set(Some(chrono::Utc::now()));

    active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(())
}

async fn update_task_progress(
    db: &Arc<DatabaseConnection>,
    task_id: &str,
    processed_files: i32,
    total_rows: i32,
    processed_rows: i32,
    success_count: i32,
    error_count: i32,
) -> Result<(), String> {
    let task = ProcessingTask::find_by_id(task_id)
        .one(db.as_ref())
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?
        .ok_or_else(|| format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))?;

    let mut active: task::ActiveModel = task.into();
    active.processed_files = Set(processed_files);
    active.total_rows = Set(total_rows);
    active.processed_rows = Set(processed_rows);
    active.success_count = Set(success_count);
    active.error_count = Set(error_count);
    active.updated_at = Set(Some(chrono::Utc::now()));

    active.update(db.as_ref()).await.map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;

    Ok(())
}

/// æš‚åœä»»åŠ¡
#[tauri::command]
pub async fn pause_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.paused.store(true, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "paused".to_string()).await
}

/// æ¢å¤ä»»åŠ¡
#[tauri::command]
pub async fn resume_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.paused.store(false, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "processing".to_string()).await
}

/// å–æ¶ˆä»»åŠ¡
#[tauri::command]
pub async fn cancel_processing_task(
    db: tauri::State<'_, Arc<DatabaseConnection>>,
    task_id: String,
) -> Result<(), String> {
    let tasks = ACTIVE_TASKS.read().await;
    if let Some(control) = tasks.get(&task_id) {
        control.cancelled.store(true, Ordering::SeqCst);
    }
    update_task_status(&db, &task_id, "cancelled".to_string()).await
}
